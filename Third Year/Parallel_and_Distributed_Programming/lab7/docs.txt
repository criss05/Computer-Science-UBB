================================================================================
LAB DOCUMENTATION: DISTRIBUTED POLYNOMIAL & BIG NUMBER MULTIPLICATION
================================================================================

This document describes the implementation of distributed polynomial multiplication 
algorithms using MPI (Message Passing Interface). It compares the performance of 
distributed approaches against standard sequential implementations and includes 
an extension for Big Number multiplication.

I. ALGORITHMS DESCRIPTION
-------------------------

1. CLASSIC ALGORITHM (O(n^2))
   - Description: This is the standard "schoolbook" multiplication. It computes 
     every possible cross-product (a_i * b_j) and sums them into the correct 
     power of x (C[i+j]).
   - Complexity: O(n^2), where n is the number of coefficients.

2. KARATSUBA ALGORITHM (O(n^log2(3)))
   - Description: A faster, recursive divide-and-conquer algorithm. Instead of 
     performing four recursive multiplications for the sub-problems, it achieves 
     the result using only three main multiplications (Z0, Z1, Z2), trading 
     multiplication complexity for simple addition/subtraction.
   - Complexity: O(n^1.58).

II. DISTRIBUTED STRATEGIES (MPI)
--------------------------------

1. DISTRIBUTED CLASSIC (O(n^2))
   - Goal: Distribute the workload of the outer loop across multiple independent 
     nodes (processes).
   - Distribution Strategy (Data Parallelism):
     * Scatter: The Master process splits Polynomial A into contiguous chunks and 
       sends one chunk to each worker process.
     * Broadcast: Polynomial B is broadcast in its entirety to all workers, as 
       every chunk of A requires access to all of B to compute the convolution.
   - Aggregation:
     * Each worker computes a partial result vector (convolution of its chunk 
       of A with B).
     * Since these partial results overlap in the final degree space, MPI_Reduce 
       (with MPI_SUM) is used to combine the distributed vectors into the final 
       result on the Master node.

2. DISTRIBUTED KARATSUBA (O(n^1.58))
   - Goal: Exploit the three independent recursive calls (Z0, Z1, Z2) to run 
     them on separate MPI ranks.
   - Distribution Strategy (Task Parallelism):
     * The algorithm utilizes a "Level 1" split to distribute the top-level 
       recursion branches:
       - Rank 1 receives A_low, B_low and computes Z0.
       - Rank 2 receives A_high, B_high and computes Z2.
       - Master (Rank 0) computes the middle term Z1 locally.
   - Hybrid Execution:
     * After the initial distribution, each node solves its assigned sub-problem 
       using the Sequential Karatsuba algorithm to avoid excessive network 
       overhead from deep recursion.

III. BONUS: BIG NUMBER MULTIPLICATION
-------------------------------------

1. CONCEPT
   - Relation to Polynomials: Multiplying two large numbers (represented as 
     vectors of digits) is mathematically equivalent to polynomial multiplication 
     evaluated at x = 10 (the base).
   - Implementation: The system reuses the exact same MPI distributed algorithms 
     (Classic and Karatsuba) described above to multiply the digit vectors.

2. NORMALIZATION (CARRY PROPAGATION)
   - Post-Processing: Unlike polynomials, big numbers require a "carry" operation.
   - Strategy: After the Master node collects the final result vector via 
     MPI_Reduce, it performs a single linear pass (O(N)) from the least 
     significant digit to the most significant digit. Any value >= 10 is divided 
     by the base; the remainder is kept while the quotient is carried over to 
     the next index.

IV. PERFORMANCE MEASUREMENTS
----------------------------

System Configuration:
- Nodes/Processes: 4 MPI Processes (1 Master + 3 Workers)
- Time Unit: Seconds (s)

1. MEDIUM CASE (N = 50,000)
---------------------------
O(n^2) Sequential:                   35.2686 s
O(n^2) Distributed (MPI) 4 nodes:    8.6751 s
O(n^2) Distributed (MPI) 8 nodes:    4.6250 s
Karatsuba Sequential:                4.4570 s
Karatsuba Distributed (MPI) 4 nodes: 1.0452 s
Karatsuba Distributed (MPI) 8 nodes: 1.0593 s

2. LARGE CASE (N = 100,000)
---------------------------
O(n^2) Sequential:                   138.3993 s
O(n^2) Distributed (MPI) 4 nodes:    37.9700 s
O(n^2) Distributed (MPI) 8 nodes:    19.0150 s
Karatsuba Sequential:                13.8188 s
Karatsuba Distributed (MPI) 4 nodes: 3.3985 s
Karatsuba Distributed (MPI) 8 nodes: 3.1960 s

3. BONUS: BIG NUMBER MULTIPLICATION (N = 100,000 Digits)
-------------------------------------------------------
O(n^2) BigNum MPI 4 nodes:           38.5889 s
O(n^2) BigNum MPI 8 nodes:           22.0665 s
Karatsuba BigNum MPI 4 nodes:        3.3416 s
Karatsuba BigNum MPI 8 nodes:        3.4098 s

================================================================================